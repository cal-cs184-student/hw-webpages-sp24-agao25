<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
  body {
    background-color: white;
    padding: 100px;
    width: 1000px;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Open Sans', sans-serif;
    color: #121212;
  }
  h1, h2, h3, h4 {
    font-family: 'Source Sans Pro', sans-serif;
  }
  kbd {
    color: #121212;
  }
</style>
<title>CS 184 Path Tracer</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
    }
  };
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>

</head>


<body>

<h1 align="middle">CS 184: Computer Graphics and Imaging, Spring 2024</h1>
<h1 align="middle">Project 3-1: Path Tracer</h1>
<h2 align="middle">Grace Liu and Alex Gao</h2>

<!-- Add Website URL -->
<h2 align="middle">Website URL: <a href="TODO">https://cal-cs184-student.github.io/hw-webpages-sp24-agao25/</a></h2>

<!--<div align="center">-->
<!--  <table style="width=100%">-->
<!--      <tr>-->
<!--          <td align="middle">-->
<!--          <img src="images/example_image.png" width="480px" />-->
<!--          <figcaption align="middle">Results Caption: my bunny is the bounciest bunny</figcaption>-->
<!--      </tr>-->
<!--  </table>-->
<!--</div>-->

<h2 align="middle">Overview</h2>
<p>
In this homework assignment, we implemented several path tracing algorithms consisting of ray generation, BVH acceleration structures, realistic and physically based lighting, and adaptive sampling. We ran into some segfault issues and infinite looping, so stepping into the debugger and using print statements was critical to evaluating our algorithms and variables. We also had to take advantage of cell render mode so as to not waste time waiting to see our rendered images and then debug the code. Understanding what the homework was asking to be implemented was also confusing at times, so drawing proper diagrams and revisiting lecture pseudo code was helpful. While this project took a lot of work and was really intensive, it was really interesting to see how we could render our own images and make them realistic by using ray tracing to replicate realistic lighting. It also was very apparent how critical optimizations in ray tracing are, especially considering how long the renders took on our own devices.

</p>
<h2 align="middle">Part 1: Ray Generation and Scene Intersection (20 Points)</h2>
<!-- Walk through the ray generation and primitive intersection parts of the rendering pipeline.
Explain the triangle intersection algorithm you implemented in your own words.
Show images with normal shading for a few small .dae files. -->

<h3>
  Walk through the ray generation and primitive intersection parts of the rendering pipeline.
</h3>
<p>
We begin the rendering pipeline by converting coordinates from image space to camera space, and then creating a local camera ray that when transformed into world space, can be utilized for our use in data structures and advanced rendering. Now that we can generate camera rays, we can specify a number of rays to be generated and track them throughout a scene. With these rays, we can also begin to estimate scene illumination. With our various camera rays, we can evaluate if they are intersecting a triangle or sphere by evaluating the intersection of the relevant vectors between the mesh and the normals. Based on the function, we will check our t parameterization, update the t_low and t_high, and then potentially identify the closest intersection. Now, we are able to begin to render scenes.
</p>

<h3>
  Explain the triangle intersection algorithm you implemented in your own words.
</h3>
<p>
In our triangle intersection algorithm, we had two functions: one to check if there was an intersection (has_intersection) and one to check for an intersection as well as update for the closest intersection (intersect). In both has_intersection and intersect, we used the Moller Trumbore algorithm to calculate when and where a ray intersects the surface.
</p>
<br>
<div align="center">
  <table style="width=100%">
      <tr>
          <td align="middle">
          <img src="results/MTAlg.png" width="480px" />
          <figcaption align="middle">Moller Trumbore Algorithm</figcaption>
      </tr>
  </table>
</div>
<p>Implementing this allows us to use position vectors from the triangle mesh and vectors that stem from the input ray to calculate 2 barycentric coordinates and the distance from the ray origin to the intersection point. We’ll update our t_max value to be t if t is between our t_min and t_max, and use the 2 calculated barycentric coordinates to find the last one. Using barycentric coordinates, we solved for the vertex normals at each of the triangle’s three vertices. In checking the dot product between position vectors and the ray, we checked if the calculated t value fit within the given bounds, and if so, updated accordingly. In intersect, we used has_intersection as a helper function and re-evaluated the triangle mesh and ray to return the intersection normal and assign the BSDF.
</p>
<p>
We also implemented sphere intersection with very similar logic to triangle intersection, but our equations are a little different now because of the circular geometry. The surface normals are also able to be calculated analytically as its the vector from the center of the sphere to the intersection point p.
</p>
<div align="center">
  <table style="width=100%">
      <tr>
          <td align="middle">
          <img src="results/RSspheres.png" width="480px" />
      </tr>
  </table>
</div>

<h3>
  Show images with normal shading for a few small .dae files.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="results/Part1/banana1.png" align="middle" width="480px"/>
        <figcaption>Banana.dae</figcaption>
      </td>
      <td>
        <img src="results/Part1/cow1.png" align="middle" width="480px"/>
        <figcaption>Cow.dae</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="results/Part1/teapot1.png" align="middle" width="480px"/>
        <figcaption>Teapot.dae</figcaption>
      </td>
      <td>
        <img src="results/Part1/peter1.png" align="middle" width="480px"/>
        <figcaption>Peter.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h2 align="middle">Part 2: Bounding Volume Hierarchy (20 Points)</h2>
<!-- Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis. -->

<h3>
  Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
</h3>
<p>
Our Bounding Volume Hierarchy (BVH) construction algorithm is reliant on the fact that a BVH is essentially a fancier binary tree. There is one root node, left and right children in between, and the finally leaf nodes at the “bottom.” The BVH construction also relies on our bounding box and how the bounding box is reduced and separated as children nodes have smaller bounding boxes that contain fewer primitives (ex. triangles). Firstly, we initialized a new BVHNode and established the bounding box of a list of primitives that we are working with. From there, we defined the heuristic for which we would split the bounding box into “left” and “right” children nodes until these nodes consisted of no more than max_leaf_size primitives, thus becoming leaf nodes. In our algorithm, we calculated the ranges of each axis of the bounding box of primitives and used various logic comparisons to determine which axis had the maximum difference. The midpoint of this axis would split the bounding box and recursively construct subtrees until we had leaf nodes. Primitives would thus be divided into smaller bounding boxes as they were grouped into smaller and smaller children nodes, eventually reaching a leaf node. Using this data structure, by examining ray intersections of the bounding box, we can quickly rule out which primitives won’t be intersected in the BVH and examine only those primitives relevant in the intersection calculations.
</p>

<h3>
  Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="results/Part2/beast2.png" align="middle" width="480px"/>
        <figcaption>beast.dae</figcaption>
      </td>
      <td>
        <img src="results/Part2/blob2.png" align="middle" width="480px"/>
        <figcaption>blob.dae</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="results/Part2/maxplanck2.png" align="middle" width="480px"/>
        <figcaption>maxplanck.dae</figcaption>
      </td>
      <td>
        <img src="results/Part2/CBlucy.png" align="middle" width="480px"/>
        <figcaption>CBlucy.dae</figcaption>
      </td>
    </tr>
  </table>
</div>

<h3>
  Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis.
</h3>
<p>
Comparing a few scenes, banana.dae took 1.334 seconds to render in part 1 without BVH acceleration but in part 2 with BVH acceleration, rendered in 0.0666 seconds. Cow.dae in part 1 took 2.8237 seconds to render while in part 2, rendered in 0.1160 seconds. In part 1, CBlucy.dae took 138.6011 seconds to render while it rendered in 0.0638 seconds in part 2. Wall-e.dae took 398.266 seconds to render in part 1 compared to 0.1917 seconds in part 2. We can clearly see that BVH acceleration greatly sped up the rendering time of various types of scene images.
</p>

<h2 align="middle">Part 3: Direct Illumination (20 Points)</h2>
<!-- Walk through both implementations of the direct lighting function.
Show some images rendered with both implementations of the direct lighting function.
Focus on one particular scene with at least one area light and compare the noise levels in soft shadows when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, not uniform hemisphere sampling.
Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis. -->

<h3>
  Walk through both implementations of the direct lighting function.
</h3>
<p>
Uniform hemisphere sampling is a method for us to evaluate a point and estimate its direct lighting by sampling the surrounding light uniformly in a hemisphere around the point. To begin, we need to calculate the reflected light after a camera ray has interacted with a point in world space and thus will estimate all the light that arrived at that point. Instead of using an integral, we can use a Monte Carlo estimate. In our algorithm, we evaluated all of the rays by evaluating intersections and making adjustments to ensure the hit point is not below the surface. This might cause us issues in summation later. Once we have a value for the amount of incoming light, we utilize the reflection equation and apply the BSDF, direction, and ratio between the incoming and outgoing rays to calculate the outgoing light. We also updated our wrapped function and global radiance estimator to include this addition of light bounce.
</p>
<p>
Importance sampling is very similar to uniform hemisphere sampling, except now we want to also sample the directions between our intersection points and the light source. Uniform hemisphere sampling gave us a nice even approximation to solve for lighting, but that resulted in noisy pictures. We can improve our results by sampling where we expect more contributions to the summation of incoming light,  looping over the pointers of light and evaluating each of their intersections with the world by creating “shadow” rays. Our calculation again will involve BSDF, direction, emittance, and also the probability distribution function we are working with. If the light is not a point light source, we need to calculate our shadows and use Monte Carlo again to evaluate the lighting for each pointer.
</p>
<h3>
  Show some images rendered with both implementations of the direct lighting function.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <!-- Header -->
    <tr align="center">
      <th>
        <b>Uniform Hemisphere Sampling</b>
      </th>
      <th>
        <b>Light Sampling</b>
      </th>
    </tr>
    <tr align="center">
      <td>
        <img src="results/Part3/Uniform/CBbunny_H_64_32.png" align="middle" width="480px"/>
        <figcaption>CBbunny.dae</figcaption>
      </td>
      <td>
        <img src="results/Part3/Importance/CBbunny_64_32_imp.png" align="middle" width="480px"/>
        <figcaption>CBbunny.dae</figcaption>
      </td>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="results/Part3/Uniform/CBspheres_H_64_32.png" align="middle" width="480px"/>
        <figcaption>CBspheres_lambertian.dae</figcaption>
      </td>
      <td>
        <img src="results/Part3/Importance/CBspheres_64_32_imp.png" align="middle" width="480px"/>
        <figcaption>CBspheres_lambertian.dae</figcaption>
      </td>
    </tr>
    <br>
  </table>
</div>
<br>

<h3>
  Focus on one particular scene with at least one area light and compare the noise levels in <b>soft shadows</b> when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, <b>not</b> uniform hemisphere sampling.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="results/Part3/Importance_light/bunny_1_1.png" align="middle" width="480px"/>
        <figcaption>1 Light Ray (Bunny.dae)</figcaption>
      </td>
      <td>
        <img src="results/Part3/Importance_light/bunny_1_4.png" align="middle" width="480px"/>
        <figcaption>4 Light Rays (Bunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="results/Part3/Importance_light/bunny_1_16.png" align="middle" width="480px"/>
        <figcaption>16 Light Rays (Bunny.dae)</figcaption>
      </td>
      <td>
        <img src="results/Part3/Importance_light/bunny_1_64.png" align="middle" width="480px"/>
        <figcaption>64 Light Rays (Bunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<p>
We can see that as the number of light rays is increased the image quality gets less noisy and better. The bunny’s shadow is also not quite correct when rendered at lower numbers of light rays but it becomes more real as more light rays blend the shadow visualization properly.
</p>

<h3>
  Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis.
</h3>
<p>
Overall, we can see that uniform hemisphere sampling results in grainier and noisier photos when directly compared to importance sampling for the same initial parameters. In only sampling the hemisphere for rays at a point, we miss the contribution of many rays since the shadow rays cast will likely not intersect a light source and thus, we see spotty dark areas and patches. With importance sampling though, we are only examining rays that directly contribute to our result at the surface and intersect with a light source. This results in better images and allows importance sampling to converge sooner to a nicer image.

</p>

<h2 align="middle">Part 4: Global Illumination (20 Points)</h2>
<!-- Walk through your implementation of the indirect lighting function.
Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
You will probably want to use the instructional machines for the above renders in order to not burn up your own computer for hours. -->

<h3>
  Walk through your implementation of the indirect lighting function.
</h3>
<p>
In global illumination, now we want to evaluate lighting not only directly from the light source(s), but also as the light bounces off other surfaces and arrives at another intersection point (indirect lighting). This will further increase the quality of our images. For up to N bounces of light, our algorithm will call zero_bounce_radiance and one_bounce_radiance. This will be summed on top of calls to at_least_one_bounce_radiance and recursively calling at_least_one_bounce_radiance until it reaches the number of bounces specified. This means that as camera rays disperse in the scene, we need to track how they might bounce in various directions and how much light is at that location given the number of bounces, light sources, etc. We have termination conditions if the ray’s depth is too small and also with some probability based on our Russian roulette. For our algorithm, the termination probability was 0.3. As the rays are continually called at intersection points, we find new ray vectors of incoming lighting by sampling new rays from the BSDF of w_out, a continuous probability distribution, and w_i in world space (transformed via the o2w matrix). If there is an intersection between the BVH and the new rays, we accumulate their radiance and account for the indirect lighting’s weight via BSDF and cosine of w_in. We’ve implemented Russian Roulette to address the fact that declaring a maximum ray depth will lead to a biased result. We also can’t iterate over infinite bounces, but Russian Roulette is purely random and probability based, giving us an unbiased probability method of termination.
</p>

<h3>
  Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="results/Part4/Examples/bunny4_3_gl.png" align="middle" width="480px"/>
        <figcaption>CBbunny.dae</figcaption>
      </td>
      <td>
        <img src="results/Part4/Examples/spheres4_3_gl.png" align="middle" width="480px"/>
        <figcaption>CBspheres_lambertian.dae</figcaption>
      </td>
    </tr>
  </table>
</div>

<h3>
  Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="results/Part4/OnlyD_Or_NoD/bunny4_3_direct.png" align="middle" width="480px"/>
        <figcaption>Only direct illumination (Bunny)</figcaption>
      </td>
      <td>
        <img src="results/Part4/OnlyD_Or_NoD/bunny4_3_indirect.png" align="middle" width="480px"/>
        <figcaption>Only indirect illumination (Bunny)</figcaption>
      </td>
    </tr>
  </table>
</div>
<p>
These pictures highlight how only some parts of the scene are captured by direct lighting or indirect lighting. For example, only direct lighting includes the light at the top of the scene while indirect lighting reveals how the ceiling and underside of the bunny are accounted for and accumulated in the indirect lighting algorithm. Because it is indirect lighting and thus there is some energy less from bouncing, the indirect lighting picture is also dimmer.
</p>

<h3>
    For CBbunny.dae, render the mth bounce of light with max_ray_depth set to 0, 1, 2, 3, 4, and 5 (the -m flag), and isAccumBounces=false. Explain in your writeup what you see for the 2nd and 3rd bounce of light, and how it contributes to the quality of the rendered image compared to rasterization. Use 1024 samples per pixel.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="results/Part4/depth_norr_noAccum/bunny4_3_0_noAc.png" align="middle" width="480px"/>
        <figcaption>max_ray_depth = 0, isAccum = false (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="results/Part4/depth_norr_noAccum/bunny4_3_1_noAc.png" align="middle" width="480px"/>
        <figcaption>max_ray_depth = 1, isAccum = false (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="results/Part4/depth_norr_noAccum/bunny4_3_2_noAc.png" align="middle" width="480px"/>
        <figcaption>max_ray_depth = 2, isAccum = false (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="results/Part4/depth_norr_noAccum/bunny4_3_3_noAc.png" align="middle" width="480px"/>
        <figcaption>max_ray_depth = 3, isAccum = false (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="results/Part4/depth_norr_noAccum/bunny4_3_4_noAc.png" align="middle" width="480px"/>
        <figcaption>max_ray_depth = 4, isAccum = false (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="results/Part4/depth_norr_noAccum/bunny4_3_5_noAc.png" align="middle" width="480px"/>
        <figcaption>max_ray_depth = 5, isAccum = false (CBbunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<p>
As the mth bounce of light increases, so does the dimness/darkness of the rendered image. Like mentioned above, the light rays do not keep 100% of the energy after intersection and bounce, and thus there is less lighting when looking solely at more bounces without accumulation. In max_ray_depth = 2, we can see how the bounced rays render the underside of the bunny visible and not shadowy. In max_ray_depth = 3, the image gets generally darker and more even in terms of where in the scene is lit up. Upon accumulation of each of these scenes at a certain amount of bounces, we’ll get a more accurate and realistic image than rasterization because we are more properly solving for how light rays will bounce from the light source to the surfaces/boundaries for various numbers of ray bounces. Rasterization would attempt to render the scene’s lighting in one attempt by discretizing the object’s shape and using logic to try to recreate the lighting effect. Therefore, the quality of the rendered image by accumulating bounces of light will be better compared to that of rasterization.
</p>

<h3>
    For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, 4, and 5(the -m flag). Use 1024 samples per pixel.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="results/Part4/depth_norr/bunny4_3_0_gl.png" align="middle" width="480px"/>
        <figcaption>max_ray_depth = 0 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="results/Part4/depth_norr/bunny4_3_1_gl.png" align="middle" width="480px"/>
        <figcaption>max_ray_depth = 1 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="results/Part4/depth_norr/bunny4_3_2_gl.png" align="middle" width="480px"/>
        <figcaption>max_ray_depth = 2 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="results/Part4/depth_norr/bunny4_3_3_gl.png" align="middle" width="480px"/>
        <figcaption>max_ray_depth = 3 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="results/Part4/depth_norr/bunny4_3_4_gl.png" align="middle" width="480px"/>
        <figcaption>max_ray_depth = 4 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="results/Part4/depth_norr/bunny4_3_5_gl.png" align="middle" width="480px"/>
        <figcaption>max_ray_depth = 5 (CBbunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<p>
The biggest difference is between max_ray_depth = 0 and then max_ray_depth = 1, but the image quickly begins to converge by around max_ray_depth = 2 or 3. After that, the contribution of more bounces becomes fainter and fainter, really only providing marginally improvements to the quality of the image.
</p>

<h3>
  For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, 4, and 100 (the -m flag). Use 1024 samples per pixel.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="results/Part4/rr/bunny4_3_rr_0.png" align="middle" width="480px"/>
        <figcaption>max_ray_depth = 0 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="results/Part4/rr/bunny4_3_rr_1.png" align="middle" width="480px"/>
        <figcaption>max_ray_depth = 1 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="results/Part4/rr/bunny4_3_rr_2.png" align="middle" width="480px"/>
        <figcaption>max_ray_depth = 2 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="results/Part4/rr/bunny4_3_rr_3.png" align="middle" width="480px"/>
        <figcaption>max_ray_depth = 3 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="results/Part4/rr/bunny4_3_rr_4.png" align="middle" width="480px"/>
        <figcaption>max_ray_depth = 4 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="results/Part4/rr/bunny4_3_rr_100.png" align="middle" width="480px"/>
        <figcaption>max_ray_depth = 100 (CBbunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<p>
We see similar results with the previous table but now with Russian Roulette, we can be sure that we are returning unbiased results since the termination of the rendering is more probability based.
</p>

<h3>
  Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="results/Part4/diff_s/bunny4_3_s1_4.png" align="middle" width="480px"/>
        <figcaption>1 sample per pixel </figcaption>
      </td>
      <td>
        <img src="results/Part4/diff_s/bunny4_3_s2_4.png" align="middle" width="480px"/>
        <figcaption>2 samples per pixel </figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="results/Part4/diff_s/bunny4_3_s4_4.png" align="middle" width="480px"/>
        <figcaption>4 samples per pixel </figcaption>
      </td>
      <td>
        <img src="results/Part4/diff_s/bunny4_3_s8_4.png" align="middle" width="480px"/>
        <figcaption>8 samples per pixel </figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="results/Part4/diff_s/bunny4_3_s16_4.png" align="middle" width="480px"/>
        <figcaption>16 samples per pixel </figcaption>
      </td>
      <td>
        <img src="results/Part4/diff_s/bunny4_3_s64_4.png" align="middle" width="480px"/>
        <figcaption>64 samples per pixel </figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="results/Part4/diff_s/bunny4_3_s1024_4.png" align="middle" width="480px"/>
        <figcaption>1024 samples per pixel </figcaption>
      </td>
    </tr>
  </table>
</div>
<p>
It is very apparent from these images that as we increase the sample rate, the graininess and noisiness of the picture decreases. We do need quite a lot of samples per pixel to address the noise, and our renders at 1024 rays per pixel took well over 20 minutes. The quality of the pictures at higher sampling rates is evidently really great.
</p>

<h2 align="middle">Part 5: Adaptive Sampling (20 Points)</h2>
<!-- Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
Pick one scene and render it with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth. -->

<h3>
  Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
</h3>
<p>
Instead of continuing to increase our sample rate to reduce noise, we can try to sample based on the convergence of each pixel and focus our sampling in areas of the image that are more difficult to render. Some pixels will not need a high sample rate, but others will so we can save computational load by understanding which situations require higher sampling rates and which don’t. To do this, we implemented a simple statistical algorithm that measures each pixel’s convergence (I = 1.96 * sigma/sqrt(n)) and checks if it exceeds a given maxTolerance * mean where sigma is the standard deviation and n is the present number of samples. If the convergence fits below this tolerance, then we can stop tracing additional rays. We were given 1.96 for the calculation of convergence, but this represents 95% confidence where the average pixel illuminance should be within mean - I and mean + I. In our implementation, we modified our raytrace_pixel from 1.2 by incorporating a convergence definition and checking inside the loop for each ray we generated. After each ray has been generated, we evaluate its lighting and also add it to our overall summation vector. Then, we use a modulo inside an if condition since it’s quite intensive to check pixel convergence for every sample, so we’ll only check every samplesPerBatch (which is given as 32). When we are evaluating convergence, we define s1 as the sum of the illuminance and s2 as the sum of the illuminance squared. This allows us to calculate the mean as s1 / n (samples so far) and sigma^2 = (1/n-1) * (s2 - s1^2/n). With a calculated standard deviation and mean, we can evaluate the convergence at the number of samples we have traversed so far and then break out if we meet the tolerance. We also need to update our samplecountBuffer with the number of samples we have traversed so far and update the pixel for the total lighting divided by the present number of samples.

</p>
<h3>
  Pick two scenes and render them with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="results/Part5/bunny5.png" align="middle" width="480px"/>
        <figcaption>Rendered image (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="results/Part5/bunny5_rate.png" align="middle" width="480px"/>
        <figcaption>Sample rate image (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="results/Part5/Blob5.png" align="middle" width="480px"/>
        <figcaption>Rendered image (Blob.dae)</figcaption>
      </td>
      <td>
        <img src="results/Part5/Blob5_Rate.png" align="middle" width="480px"/>
        <figcaption>Sample rate image (Blob.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<p> Collaboration: Grace was the main implementer of parts 1-4 while Alex helped debug those parts and implemented part 5. Alex conducted all of the experiments and prepared the writeup.</p>

</body>
</html>
